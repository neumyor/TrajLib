{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>POLYLINE</th></tr><tr><td>list[array[f64, 2]]</td></tr></thead><tbody><tr><td>[[-8.610291, 41.140746], [-8.6103, 41.140755], … [-8.60589, 41.145345]]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1,)\n",
       "Series: 'POLYLINE' [list[array[f64, 2]]]\n",
       "[\n",
       "\t[[-8.610291, 41.140746], [-8.6103, 41.140755], … [-8.60589, 41.145345]]\n",
       "]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# 读取 CSV 文件，过滤掉 MISSING_DATA 为 True 的行，按 TIMESTAMP 排序，选取 POLYLINE 列，取前 1000 个值\n",
    "original_trajs = (\n",
    "    pl.read_csv(\"./resource/dataset/Porto/porto_sample.csv\")\n",
    "    .filter(pl.col(\"MISSING_DATA\") == False)\n",
    "    .sort(\"TIMESTAMP\")[\"POLYLINE\"]\n",
    "    .limit(1000)\n",
    "    .map_elements(lambda x: np.array(eval(x)), return_dtype=pl.List(pl.Array(pl.Float64, 2)))\n",
    ")\n",
    "original_trajs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 490.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from fedtraj.utils.trajclus import traclus_partition\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def cut_trajectorys_into_segments(original_trajs):\n",
    "    new_data = []\n",
    "    # 遍历 POLYLINE 列中的每个轨迹\n",
    "    for traj in tqdm(original_trajs):\n",
    "        # 调用 traj_clus 函数得到切分点布尔数组\n",
    "        _, split_points = traclus_partition(traj)\n",
    "\n",
    "        # 找到所有切分点的索引\n",
    "        split_indices = np.where(split_points)[0]\n",
    "\n",
    "        # 处理没有切分点的情况\n",
    "        if len(split_indices) == 0:\n",
    "            new_trajlen = len(traj)\n",
    "            new_polyline = traj\n",
    "            if new_trajlen > 0:  # 仅添加长度不为 0 的轨迹\n",
    "                new_data.append([new_trajlen, new_polyline])\n",
    "        else:\n",
    "            # 切分轨迹\n",
    "            for i in range(len(split_indices) - 1):\n",
    "                start = split_indices[i]\n",
    "                end = split_indices[i + 1]\n",
    "                new_trajlen = end - start + 1\n",
    "                new_polyline = traj[start: end + 1]\n",
    "                if new_trajlen > 0:  # 仅添加长度不为 0 的轨迹\n",
    "                    new_data.append([new_trajlen, new_polyline])\n",
    "\n",
    "    # 转换为适合创建 Polars DataFrame 的格式\n",
    "    trajlen_list = [item[0] for item in new_data]\n",
    "    polyline_list = [item[1] for item in new_data]\n",
    "\n",
    "    # 创建新的 Polars DataFrame\n",
    "    new_df = pl.DataFrame({\"POLYLINE\": polyline_list})[\"POLYLINE\"]\n",
    "    return new_df\n",
    "\n",
    "\n",
    "cut_trajs = cut_trajectorys_into_segments(original_trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15686"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cut_trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90a94423a1046d58eb069b13499751b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transform trajectorys:   0%|          | 0/15686 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory(entity_id=0, length=4) [[-8.610291 41.140746]\n",
      " [-8.6103   41.140755]\n",
      " [-8.610309 41.14089 ]\n",
      " [-8.613657 41.141358]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.contrib import tenumerate\n",
    "from trajdl.datasets import Trajectory\n",
    "\n",
    "all_trajs = [\n",
    "    Trajectory(traj_pl.to_numpy(), entity_id=str(idx))\n",
    "    for idx, traj_pl in tenumerate(cut_trajs, desc=\"transform trajectorys\")\n",
    "]\n",
    "print(all_trajs[0], all_trajs[0].seq[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15686"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing training set: 100%|██████████| 12548/12548 [00:00<00:00, 941943.06it/s]\n",
      "Constructing validation set: 100%|██████████| 1568/1568 [00:00<00:00, 1389534.90it/s]\n",
      "Constructing test set: 100%|██████████| 1570/1570 [00:00<00:00, 1225582.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 12548\n",
      "Validation set length: 1568\n",
      "Test set length: 1570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_trajectories(all_trajs, min_length_train_val=2, max_length_train_val=64, min_length_test=2, max_length_test=64):\n",
    "    \"\"\"\n",
    "    此函数用于将轨迹数据划分为训练集、验证集和测试集。\n",
    "\n",
    "    参数:\n",
    "    all_trajs (list): 包含所有轨迹数据的列表。\n",
    "    min_length_train_val (int): 训练集和验证集轨迹的最小长度。\n",
    "    max_length_train_val (int): 训练集和验证集轨迹的最大长度。\n",
    "    min_length_test (int): 测试集轨迹的最小长度。\n",
    "    max_length_test (int): 测试集轨迹的最大长度。\n",
    "\n",
    "    返回:\n",
    "    tuple: 包含训练集、验证集和测试集的元组。\n",
    "    \"\"\"\n",
    "    # 获取所有轨迹的长度\n",
    "    length = len(all_trajs)\n",
    "    # 计算训练集、验证集和测试集的大小\n",
    "    num_train = int(0.8 * length)\n",
    "    num_val = int(0.1 * length)\n",
    "    num_test = length - num_train - num_val\n",
    "\n",
    "    # 初始化训练集、验证集和测试集列表\n",
    "    train_traj, val_traj, test_traj = [], [], []\n",
    "\n",
    "    # 划分训练集\n",
    "    for traj in tqdm(all_trajs[:num_train], desc=\"Constructing training set\"):\n",
    "        # 检查轨迹长度是否在训练集要求的范围内\n",
    "        if min_length_train_val <= len(traj) <= max_length_train_val:\n",
    "            train_traj.append(traj)\n",
    "\n",
    "    # 划分验证集\n",
    "    for traj in tqdm(all_trajs[num_train:num_train + num_val], desc=\"Constructing validation set\"):\n",
    "        # 检查轨迹长度是否在验证集要求的范围内\n",
    "        if min_length_train_val <= len(traj) <= max_length_train_val:\n",
    "            val_traj.append(traj)\n",
    "\n",
    "    # 划分测试集\n",
    "    for traj in tqdm(all_trajs[num_train + num_val:], desc=\"Constructing test set\"):\n",
    "        # 检查轨迹长度是否在测试集要求的范围内\n",
    "        if min_length_test <= len(traj) <= max_length_test:\n",
    "            test_traj.append(traj)\n",
    "\n",
    "    return train_traj, val_traj, test_traj\n",
    "\n",
    "# 假设 all_trajs 是包含所有轨迹数据的列表\n",
    "# all_trajs = [...]\n",
    "\n",
    "# 调用函数进行数据划分\n",
    "train_traj, val_traj, test_traj = split_trajectories(all_trajs)\n",
    "\n",
    "# 打印各数据集的长度\n",
    "print(f\"Training set length: {len(train_traj)}\")\n",
    "print(f\"Validation set length: {len(val_traj)}\")\n",
    "print(f\"Test set length: {len(test_traj)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boundary_original: RectangleBoundary(min_x=-8.690261, min_y=41.140092, max_x=-8.549155, max_y=41.185969)\n",
      "boundary: RectangleBoundary(min_x=-967395.429381, min_y=5033027.213480, max_x=-951687.581313, max_y=5039810.867670)\n"
     ]
    }
   ],
   "source": [
    "from trajdl import trajdl_cpp\n",
    "\n",
    "# 基于经纬度系统的区域边界\n",
    "boundary_original = trajdl_cpp.RectangleBoundary(\n",
    "    min_x=-8.690261,\n",
    "    min_y=41.140092,\n",
    "    max_x=-8.549155,\n",
    "    max_y=41.185969,\n",
    ")\n",
    "# 转换为基于平面坐标系的区域边界\n",
    "boundary = boundary_original.to_web_mercator()\n",
    "print(f\"boundary_original: {boundary_original}\")\n",
    "print(f\"boundary: {boundary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebMercatorCoord(x=-967395.4293806, y=5033027.2134798)\n",
      "WebMercatorCoord(x=0.0000000, y=-0.0000000)\n"
     ]
    }
   ],
   "source": [
    "print(trajdl_cpp.convert_gps_to_webmercator(-8.690261, 41.140092))\n",
    "print(trajdl_cpp.convert_gps_to_webmercator(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niuyiming/.conda/envs/trajlib/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10744 158 68\n"
     ]
    }
   ],
   "source": [
    "from trajdl.grid import SimpleGridSystem\n",
    "\n",
    "# 网格的划分距离为100m\n",
    "grid_width, grid_height = 100, 100\n",
    "\n",
    "# 创建网格系统\n",
    "grid = SimpleGridSystem(\n",
    "    # 使用波尔多市的左下角点和右上角点来构建和切分网格系统\n",
    "    boundary,\n",
    "    step_x=grid_width,\n",
    "    step_y=grid_height,\n",
    ")\n",
    "print(len(grid), grid.num_x_grids, grid.num_y_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebMercatorCoord(x=-958493.2097019, y=5033123.8845711) 89\n"
     ]
    }
   ],
   "source": [
    "# 转墨卡托坐标系统\n",
    "web_mercator_location = trajdl_cpp.convert_gps_to_webmercator(-8.610291, 41.140746)\n",
    "# 转网格id\n",
    "x, y = web_mercator_location.x, web_mercator_location.y\n",
    "grid_id = grid.locate_unsafe(x, y)\n",
    "\n",
    "print(web_mercator_location, grid_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num vocab:  113\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from trajdl.tokenizers.t2vec import T2VECTokenizer\n",
    "\n",
    "output_folder = \"./output/t2vec\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "tokenizer = T2VECTokenizer.build(\n",
    "    grid=grid,\n",
    "    boundary=boundary_original,\n",
    "    trajectories=all_trajs,\n",
    "    max_vocab_size=40000,  # 词表支持的词元上限，排序逻辑是词元在数据集中的频率\n",
    "    min_freq=64,  # 被命中至少min_freq次的网格称之为`hot cell`，tokenizer中仅保留`hot cell`\n",
    "    with_kd_tree=True,\n",
    ")\n",
    "tokenizer.save_pretrained(os.path.join(output_folder, \"tokenizer.pkl\"))\n",
    "print(\"num vocab: \", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trajdl.common.enum import TokenEnum\n",
    "\n",
    "k = 10  # 10个最近的网格\n",
    "SPECIAL_TOKENS = TokenEnum.values()\n",
    "vocab_list = tokenizer.vocab.keys()  # 获取全部字典的Token\n",
    "loc_list, idx_list = zip(\n",
    "    *((loc, tokenizer.loc2idx(loc)) for loc in vocab_list if loc not in SPECIAL_TOKENS)\n",
    ")  # 剔除special tokens字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dists, locations = tokenizer.k_nearest_hot_loc(\n",
    "    loc_list, k=k\n",
    ")  # 获取k个最近的网格以及对应的距离\n",
    "\n",
    "# (num_locations, k)，索引矩阵\n",
    "V = np.zeros(shape=(len(vocab_list), k), dtype=np.int64)\n",
    "\n",
    "# (num_locations, k)，距离矩阵\n",
    "D = np.zeros_like(V, dtype=np.float32)\n",
    "D[idx_list, :] = dists\n",
    "\n",
    "# 对于SPECIAL TOKENS，最近的token设定为自己\n",
    "for token in SPECIAL_TOKENS:\n",
    "    idx = tokenizer.loc2idx(token)\n",
    "    V[idx] = idx\n",
    "\n",
    "for line_idx, loc_list in zip(idx_list, locations):\n",
    "    V[line_idx] = [tokenizer.loc2idx(loc) for loc in loc_list]\n",
    "\n",
    "np.save(os.path.join(output_folder, \"knn_indices.npy\"), V)  # 保存k近邻网格的索引\n",
    "np.save(os.path.join(output_folder, \"knn_distances.npy\"), D)  # 保留k近邻网格的距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trajdl.datasets import TrajectoryDataset\n",
    "from trajdl.datasets.modules.t2vec import T2VECDataModuleV2\n",
    "\n",
    "# 将Trajectory转换为TrajectoryDataset\n",
    "train_dataset = TrajectoryDataset.init_from_trajectories(train_traj)\n",
    "val_dataset = TrajectoryDataset.init_from_trajectories(val_traj)\n",
    "test_dataset = TrajectoryDataset.init_from_trajectories(test_traj)\n",
    "\n",
    "data_module = T2VECDataModuleV2(\n",
    "    tokenizer=tokenizer,\n",
    "    train_table=train_dataset,\n",
    "    val_table=val_dataset,\n",
    "    test_table=test_dataset,\n",
    "    train_batch_size=4,\n",
    "    val_batch_size=4,\n",
    "    num_train_batches=10,\n",
    "    num_val_batches=10,\n",
    "    num_cpus=-1,\n",
    "    k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T2VECSample(src=tensor([[110, 112, 112, 112],\n",
       "        [110, 112, 112, 112],\n",
       "        [ 44,  44,  44, 112],\n",
       "        [ 44,  44,  44,  44]]), lengths=(1, 1, 3, 4), target=tensor([[108, 110, 109, 112, 112],\n",
       "        [108, 110, 109, 112, 112],\n",
       "        [108,  44,  44,  44, 109],\n",
       "        [108,  44,  44,  44, 109]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.setup(\"fit\")\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T2VEC(\n",
       "  (embedding): SimpleEmbedding(\n",
       "    (embedding): Embedding(113, 256, padding_idx=112)\n",
       "  )\n",
       "  (encoder): T2VECEncoder(\n",
       "    (emb): SimpleEmbedding(\n",
       "      (embedding): Embedding(113, 256, padding_idx=112)\n",
       "    )\n",
       "    (encoder): GRU(256, 256, batch_first=True)\n",
       "  )\n",
       "  (decoder): DecoderWithAttention(\n",
       "    (embedding_layer): SimpleEmbedding(\n",
       "      (embedding): Embedding(113, 256, padding_idx=112)\n",
       "    )\n",
       "    (rnn): StackingGRU(\n",
       "      (grus): ModuleList(\n",
       "        (0): GRUCell(256, 256)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (attention): GlobalAttention(\n",
       "      (L1): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (L2): Linear(in_features=512, out_features=256, bias=False)\n",
       "      (softmax): Softmax(dim=1)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (projector): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=113, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       "  (loss_fn): KLDivLoss()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trajdl.algorithms.t2vec import T2VEC\n",
    "\n",
    "# 构建模型，我们使用默认参数，用户也可以根据文档修改模型的类型，比如使用GRU、LSTM等编码器\n",
    "model = T2VEC(\n",
    "    embedding_dim=256,\n",
    "    hidden_size=256,\n",
    "    tokenizer=tokenizer,\n",
    "    knn_indices_path=os.path.join(output_folder, \"knn_indices.npy\"),\n",
    "    knn_distances_path=os.path.join(output_folder, \"knn_distances.npy\"),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type                 | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | embedding | SimpleEmbedding      | 28.9 K | train\n",
      "1 | encoder   | T2VECEncoder         | 423 K  | train\n",
      "2 | decoder   | DecoderWithAttention | 620 K  | train\n",
      "3 | projector | Sequential           | 29.0 K | train\n",
      "4 | loss_fn   | KLDivLoss            | 0      | train\n",
      "-----------------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "2.3 K     Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.185     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c86b3b1e4d422d90b535461673229e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fb30e1500a4c8980c893d965243ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612445db97fd417e8dbd15d088ec356a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19883f52d4cd4a7fba4a4d44399a3de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c776ad546b8544fea592c617851c5546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03abd2f56e7426ebc1eb04cd864ccca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ea2f82e8c743e38a3ed5c285eedc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6a5eb3771f400bb97f9afb84265e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334f58856b694d76bf2bcaf78454f9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1632daed4684532954162ce99b1d7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "trainer = L.Trainer(max_epochs=8, logger=False, enable_checkpointing=False)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 256) (4, 256)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 先获取test_dataloader\n",
    "data_module.setup(\"test\")\n",
    "test_dataloader = data_module.test_dataloader()\n",
    "test_sample_1 = next(iter(test_dataloader))\n",
    "test_sample_2 = next(iter(test_dataloader))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    vec_1 = model(test_sample_1)\n",
    "    vec_2 = model(test_sample_2)\n",
    "\n",
    "batch_vec_1, batch_vec_2 = vec_1.detach().cpu().numpy(), vec_2.detach().cpu().numpy()\n",
    "print(batch_vec_1.shape, batch_vec_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.070461  2.5555851 7.8009853 6.837785 ]\n",
      " [6.4445977 2.4364927 7.777173  6.776062 ]\n",
      " [6.070461  2.5555851 7.8009853 6.837785 ]\n",
      " [1.3182648 6.4440093 8.017162  7.0369325]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "print(euclidean_distances(batch_vec_1, batch_vec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculating embedding: 100%|██████████| 491/491 [00:01<00:00, 321.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15686, 256]) 15686 15686\n"
     ]
    }
   ],
   "source": [
    "from trajdl.datasets.modules.t2vec import generate_samples\n",
    "from trajdl.common.samples import T2VECSample\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假设 batch_size 是我们指定的批量大小\n",
    "batch_size = 32\n",
    "\n",
    "samples = []\n",
    "\n",
    "for traj in all_trajs:\n",
    "    sample = generate_samples(\n",
    "        tokenizer, traj\n",
    "    )\n",
    "    samples.append(sample)\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "# 按 batch_size 对 samples 进行分批处理\n",
    "for start_idx in tqdm(range(0, len(samples), batch_size), desc=\"calculating embedding\"):\n",
    "    end_idx = min(start_idx + batch_size, len(samples))\n",
    "    batch_samples = samples[start_idx:end_idx]\n",
    "    src_samples, src_lens, trg_samples = zip(*batch_samples)\n",
    "\n",
    "    # 创建当前批次的 T2VECSample 对象\n",
    "    batch = T2VECSample(\n",
    "        src=pad_sequence(src_samples, batch_first=True, padding_value=tokenizer.pad),\n",
    "        lengths=src_lens,\n",
    "        target=pad_sequence(trg_samples, batch_first=True, padding_value=tokenizer.pad),\n",
    "    )\n",
    "\n",
    "    # 进行推理\n",
    "    batch_output = model(batch)\n",
    "\n",
    "    # 将当前批次的输出添加到 all_outputs 列表中\n",
    "    all_outputs.append(batch_output)\n",
    "\n",
    "# 将所有批次的输出拼接起来得到完整的结果\n",
    "all_embeddings = torch.cat(all_outputs, dim=0)\n",
    "\n",
    "print(all_embeddings.shape, len(all_trajs), len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"./resource/results/t2vec/trajs.pkl\", 'wb') as f:\n",
    "    pickle.dump(all_trajs, f)\n",
    "\n",
    "# 使用 torch 保存嵌入\n",
    "torch.save(all_embeddings, \"./resource/results/t2vec/embs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算距离矩阵: 100%|██████████| 123017455/123017455 [1:34:56<00:00, 21594.81it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 175\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[43mrun_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 163\u001b[0m, in \u001b[0;36mrun_clustering\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m         distance_matrix[i, j] \u001b[38;5;241m=\u001b[39m dist\n\u001b[1;32m    161\u001b[0m         distance_matrix[j, i] \u001b[38;5;241m=\u001b[39m dist\n\u001b[0;32m--> 163\u001b[0m silhouette_avg \u001b[38;5;241m=\u001b[39m \u001b[43msilhouette_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistance_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecomputed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSilhouette Coefficient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msilhouette_avg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# 输出结果\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/trajlib/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/trajlib/lib/python3.10/site-packages/sklearn/metrics/cluster/_unsupervised.py:139\u001b[0m, in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         X, labels \u001b[38;5;241m=\u001b[39m X[indices], labels[indices]\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43msilhouette_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/trajlib/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/trajlib/lib/python3.10/site-packages/sklearn/metrics/cluster/_unsupervised.py:297\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    295\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m    296\u001b[0m label_freqs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(labels)\n\u001b[0;32m--> 297\u001b[0m \u001b[43mcheck_number_of_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    300\u001b[0m reduce_func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    301\u001b[0m     _silhouette_reduce, labels\u001b[38;5;241m=\u001b[39mlabels, label_freqs\u001b[38;5;241m=\u001b[39mlabel_freqs\n\u001b[1;32m    302\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/trajlib/lib/python3.10/site-packages/sklearn/metrics/cluster/_unsupervised.py:36\u001b[0m, in \u001b[0;36mcheck_number_of_labels\u001b[0;34m(n_labels, n_samples)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Number of samples.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m n_labels \u001b[38;5;241m<\u001b[39m n_samples:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of labels is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. Valid values are 2 to n_samples - 1 (inclusive)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;241m%\u001b[39m n_labels\n\u001b[1;32m     39\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from easydict import EasyDict\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from math import sqrt, atan2, sin\n",
    "\n",
    "\n",
    "class Segment(EasyDict):\n",
    "    def __init__(self, segment_id, points, emb):\n",
    "        super().__init__()\n",
    "        self.id = segment_id\n",
    "        self.points = points\n",
    "        self.emb = emb\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, segments):\n",
    "        self.items = segments\n",
    "        self.size = len(segments)\n",
    "        self.centroid = self._calculate_centroid()\n",
    "        self.radius = self._calculate_radius()\n",
    "        self.merged = False\n",
    "\n",
    "    def _calculate_centroid(self):\n",
    "        total_x = 0\n",
    "        total_y = 0\n",
    "        for seg in self.items:\n",
    "            start, end = seg.points[0], seg.points[-1]\n",
    "            mid_x = (start[0] + end[0]) / 2\n",
    "            mid_y = (start[1] + end[1]) / 2\n",
    "            total_x += mid_x\n",
    "            total_y += mid_y\n",
    "        centroid_x = total_x / self.size\n",
    "        centroid_y = total_y / self.size\n",
    "        return (centroid_x, centroid_y)\n",
    "\n",
    "    def _calculate_radius(self):\n",
    "        max_distance = 0\n",
    "        for seg in self.items:\n",
    "            start, end = seg.points[0], seg.points[-1]\n",
    "            mid_x = (start[0] + end[0]) / 2\n",
    "            mid_y = (start[1] + end[1]) / 2\n",
    "            distance = self.compute_point_distance((mid_x, mid_y), self.centroid)\n",
    "            if distance > max_distance:\n",
    "                max_distance = distance\n",
    "        return max_distance\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_point_distance(p1, p2):\n",
    "        return sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
    "\n",
    "\n",
    "def compute_angular_distance(seg1, seg2):\n",
    "    start1, end1 = seg1.points[0], seg1.points[-1]\n",
    "    start2, end2 = seg2.points[0], seg2.points[-1]\n",
    "    vector1 = (end1[0] - start1[0], end1[1] - start1[1])\n",
    "    vector2 = (end2[0] - start2[0], end2[1] - start2[1])\n",
    "    angle1 = atan2(vector1[1], vector1[0])\n",
    "    angle2 = atan2(vector2[1], vector2[0])\n",
    "    angle_diff = abs(angle1 - angle2)\n",
    "    if angle_diff > np.pi:\n",
    "        angle_diff = 2 * np.pi - angle_diff\n",
    "    len1 = Cluster.compute_point_distance(start1, end1)\n",
    "    len2 = Cluster.compute_point_distance(start2, end2)\n",
    "    return abs(sin(angle_diff)) * max(len1, len2)\n",
    "\n",
    "\n",
    "def compute_vector_distance(r1, r2):\n",
    "    sum_square = torch.sum((r1 - r2) ** 2)\n",
    "    return torch.sqrt(sum_square).item()\n",
    "\n",
    "\n",
    "def calculate_distance(seg1, seg2, alpha, beta, gamma):\n",
    "    d1 = Cluster.compute_point_distance(\n",
    "        seg1.points[0], seg2.points[0]\n",
    "    ) + Cluster.compute_point_distance(seg1.points[-1], seg2.points[-1])\n",
    "    d2 = compute_angular_distance(seg1, seg2)\n",
    "    d3 = compute_vector_distance(seg1.emb, seg2.emb)\n",
    "    return alpha * d1 + beta * d2 + gamma * d3\n",
    "\n",
    "\n",
    "def local_clustering(trajectory_segments, eps, min_samples, alpha, beta, gamma):\n",
    "    num_segments = len(trajectory_segments)\n",
    "    distance_matrix = np.zeros((num_segments, num_segments))\n",
    "    # 创建 tqdm 进度条\n",
    "    total_iterations = num_segments * (num_segments - 1) // 2\n",
    "    progress_bar = tqdm(total=total_iterations, desc=\"计算距离矩阵\")\n",
    "    count = 0\n",
    "    for i in range(num_segments):\n",
    "        for j in range(i + 1, num_segments):\n",
    "            dist = calculate_distance(\n",
    "                trajectory_segments[i], trajectory_segments[j], alpha, beta, gamma\n",
    "            )\n",
    "            distance_matrix[i, j] = dist\n",
    "            distance_matrix[j, i] = dist\n",
    "            # 更新进度条\n",
    "            count += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=\"precomputed\")\n",
    "    labels = db.fit_predict(distance_matrix)\n",
    "\n",
    "    local_clusters = []\n",
    "    unique_labels = set(labels)\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            continue\n",
    "        cluster_indices = np.where(labels == label)[0]\n",
    "        cluster_segments = [trajectory_segments[i] for i in cluster_indices]\n",
    "        cluster = Cluster(cluster_segments)\n",
    "        local_clusters.append(cluster)\n",
    "\n",
    "    return local_clusters\n",
    "\n",
    "\n",
    "def run_clustering():\n",
    "    trajs_filepath = r\"./resource/results/t2vec/trajs.pkl\"\n",
    "    embs_filepath = r\"./resource/results/t2vec/embs.pkl\"\n",
    "\n",
    "    with open(trajs_filepath, \"rb\") as f:\n",
    "        all_trajs = pickle.load(f)\n",
    "\n",
    "    # 使用 torch 加载嵌入\n",
    "    all_embeddings = torch.load(embs_filepath)\n",
    "\n",
    "    all_segments = [\n",
    "        Segment(i, traj, emb)\n",
    "        for i, (traj, emb) in enumerate(zip(all_trajs, all_embeddings))\n",
    "    ]\n",
    "\n",
    "    # 本地聚类参数\n",
    "    local_eps = 1000.0\n",
    "    local_min_samples = 2\n",
    "    alpha = 1\n",
    "    beta = 1\n",
    "    gamma = 1\n",
    "\n",
    "    # 进行集中式聚类\n",
    "    local_clusters = local_clustering(\n",
    "        all_segments, local_eps, local_min_samples, alpha, beta, gamma\n",
    "    )\n",
    "\n",
    "    # 计算轮廓系数\n",
    "    all_seg = []\n",
    "    labels = []\n",
    "    for i, cluster in enumerate(local_clusters):\n",
    "        for segment in cluster.items:\n",
    "            all_seg.append(segment)\n",
    "            labels.append(i)\n",
    "\n",
    "    num_segments = len(all_seg)\n",
    "    distance_matrix = np.zeros((num_segments, num_segments))\n",
    "    for i in range(num_segments):\n",
    "        for j in range(i + 1, num_segments):\n",
    "            dist = calculate_distance(all_seg[i], all_seg[j], alpha, beta, gamma)\n",
    "            distance_matrix[i, j] = dist\n",
    "            distance_matrix[j, i] = dist\n",
    "\n",
    "    silhouette_avg = silhouette_score(distance_matrix, labels, metric=\"precomputed\")\n",
    "    print(f\"Silhouette Coefficient: {silhouette_avg}\")\n",
    "\n",
    "    # 输出结果\n",
    "    for i, cluster in enumerate(local_clusters):\n",
    "        print(f\"Cluster {i}:\")\n",
    "        print(f\"  Centroid: {cluster.centroid}\")\n",
    "        print(f\"  Radius: {cluster.radius}\")\n",
    "        print(f\"  Size: {cluster.size}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_clustering()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
