{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>POLYLINE</th></tr><tr><td>list[array[f64, 2]]</td></tr></thead><tbody><tr><td>[[-8.610291, 41.140746], [-8.6103, 41.140755], … [-8.60589, 41.145345]]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1,)\n",
       "Series: 'POLYLINE' [list[array[f64, 2]]]\n",
       "[\n",
       "\t[[-8.610291, 41.140746], [-8.6103, 41.140755], … [-8.60589, 41.145345]]\n",
       "]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# 读取 CSV 文件，过滤掉 MISSING_DATA 为 True 的行，按 TIMESTAMP 排序，选取 POLYLINE 列，取前 1000 个值\n",
    "original_trajs = (\n",
    "    pl.read_csv(\"./resource/dataset/Porto/porto_sample.csv\")\n",
    "    .filter(pl.col(\"MISSING_DATA\") == False)\n",
    "    .sort(\"TIMESTAMP\")[\"POLYLINE\"]\n",
    "    .limit(1000)\n",
    "    .map_elements(lambda x: np.array(eval(x)), return_dtype=pl.List(pl.Array(pl.Float64, 2)))\n",
    ")\n",
    "original_trajs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 496.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15686"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from fedtraj.utils.trajclus import traclus_partition\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def cut_trajectorys_into_segments(original_trajs):\n",
    "    new_data = []\n",
    "    # 遍历 POLYLINE 列中的每个轨迹\n",
    "    for traj in tqdm(original_trajs):\n",
    "        # 调用 traj_clus 函数得到切分点布尔数组\n",
    "        _, split_points = traclus_partition(traj)\n",
    "\n",
    "        # 找到所有切分点的索引\n",
    "        split_indices = np.where(split_points)[0]\n",
    "\n",
    "        # 处理没有切分点的情况\n",
    "        if len(split_indices) == 0:\n",
    "            new_trajlen = len(traj)\n",
    "            new_polyline = traj\n",
    "            if new_trajlen > 0:  # 仅添加长度不为 0 的轨迹\n",
    "                new_data.append([new_trajlen, new_polyline])\n",
    "        else:\n",
    "            # 切分轨迹\n",
    "            for i in range(len(split_indices) - 1):\n",
    "                start = split_indices[i]\n",
    "                end = split_indices[i + 1]\n",
    "                new_trajlen = end - start + 1\n",
    "                new_polyline = traj[start: end + 1]\n",
    "                if new_trajlen > 0:  # 仅添加长度不为 0 的轨迹\n",
    "                    new_data.append([new_trajlen, new_polyline])\n",
    "\n",
    "    # 转换为适合创建 Polars DataFrame 的格式\n",
    "    trajlen_list = [item[0] for item in new_data]\n",
    "    polyline_list = [item[1] for item in new_data]\n",
    "\n",
    "    # 创建新的 Polars DataFrame\n",
    "    new_df = pl.DataFrame({\"POLYLINE\": polyline_list})[\"POLYLINE\"]\n",
    "    return new_df\n",
    "\n",
    "\n",
    "cut_trajs = cut_trajectorys_into_segments(original_trajs)\n",
    "len(cut_trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算 LCSS 距离矩阵:   0%|          | 0/123017455 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算 LCSS 距离矩阵:  10%|▉         | 11898028/123017455 [06:40<1:20:25, 23025.78it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import traj_dist.distance as tdist\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_lcss_distance_matrix(trajectories):\n",
    "    \"\"\"\n",
    "    计算轨迹之间的 LCSS 距离矩阵\n",
    "    :param trajectories: 包含 N 个 (L, 2) 数组的序列，每个数组表示一个二维轨迹\n",
    "    :return: 距离矩阵，形状为 (N, N)\n",
    "    \"\"\"\n",
    "    N = len(trajectories)\n",
    "    # 初始化距离矩阵\n",
    "    dist_matrix = np.zeros((N, N))\n",
    "    # 计算总的循环次数\n",
    "    total_iterations = N * (N - 1) // 2\n",
    "    # 创建 tqdm 进度条\n",
    "    progress_bar = tqdm(total=total_iterations, desc=\"计算 LCSS 距离矩阵\")\n",
    "    count = 0\n",
    "    # 遍历所有轨迹对\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            # 计算轨迹 i 和轨迹 j 之间的 LCSS 距离\n",
    "            dist_matrix[i, j] = tdist.lcss(np.array(trajectories[i]), np.array(trajectories[j]))\n",
    "            # 距离矩阵是对称的\n",
    "            dist_matrix[j, i] = dist_matrix[i, j]\n",
    "            # 更新进度条\n",
    "            count += 1\n",
    "            progress_bar.update(1)\n",
    "    # 关闭进度条\n",
    "    progress_bar.close()\n",
    "    return dist_matrix\n",
    "\n",
    "def cluster_trajectories(trajectories, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    基于 LCSS 距离对轨迹进行 DBSCAN 聚类，并计算凝聚系数\n",
    "    :param trajectories: 包含 N 个 (L, 2) 数组的序列，每个数组表示一个二维轨迹\n",
    "    :param eps: DBSCAN 的邻域半径\n",
    "    :param min_samples: DBSCAN 的最小样本数\n",
    "    :return: 聚类标签，凝聚系数\n",
    "    \"\"\"\n",
    "    # 计算距离矩阵\n",
    "    dist_matrix = compute_lcss_distance_matrix(trajectories)\n",
    "    # 创建 DBSCAN 聚类器\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "    # 进行聚类\n",
    "    labels = dbscan.fit_predict(dist_matrix)\n",
    "    try:\n",
    "        # 计算凝聚系数\n",
    "        silhouette_coef = silhouette_score(dist_matrix, labels, metric='precomputed')\n",
    "    except ValueError:\n",
    "        # 如果只有一个聚类或所有样本都是噪声点，则凝聚系数为 0\n",
    "        silhouette_coef = 0\n",
    "    return labels, silhouette_coef\n",
    "\n",
    "\n",
    "# 进行聚类并计算凝聚系数\n",
    "labels, silhouette_coef = cluster_trajectories(cut_trajs)\n",
    "print(\"聚类标签:\", labels)\n",
    "print(\"凝聚系数:\", silhouette_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
