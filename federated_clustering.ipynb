{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from trajlib.data_processing.utils.data_definition import TrajectoryData\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "results_path = r\"resource\\results\\test\"\n",
    "\n",
    "fed_folders = [f for f in os.listdir(results_path) if os.path.isdir(os.path.join(results_path, f)) and f.startswith('fed_')]\n",
    "all_info = []\n",
    "for i in range(len(fed_folders)):\n",
    "    fed_path = os.path.join(results_path, f\"fed_{i+1}\")\n",
    "    embs_path = os.path.join(fed_path, \"embs.pt\")\n",
    "    segments_path = os.path.join(fed_path, \"segments.pkl\")\n",
    "    embs = torch.load(embs_path, weights_only=True)\n",
    "    segments = pd.read_pickle(segments_path)\n",
    "    all_info.append({'embs': embs,'segments': segments})\n",
    "\n",
    "num_fed = len(all_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2086\n",
      "2005\n",
      "2204\n",
      "2385\n",
      "2696\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "\n",
    "\n",
    "class Segment(EasyDict):\n",
    "    def __init__(self, segment_id, points, emb):\n",
    "        super().__init__()\n",
    "        self.id = segment_id\n",
    "        self.points = points\n",
    "        self.emb = emb\n",
    "\n",
    "\n",
    "fed_segments = []\n",
    "for fed_id in range(num_fed):\n",
    "    traj_data = all_info[fed_id]['segments']\n",
    "    \n",
    "    segments = []\n",
    "    for i, row in traj_data.iterrows():\n",
    "        points = row['merc_seq']\n",
    "        if len(points) < 2:\n",
    "            continue\n",
    "        segment_id = i\n",
    "        emb = all_info[fed_id]['embs'][i]\n",
    "        segment = Segment(segment_id, points, emb)\n",
    "        segments.append(segment)\n",
    "\n",
    "    fed_segments.append(segments)\n",
    "\n",
    "for client in fed_segments:\n",
    "    print(len(client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_point_distance(p1, p2):\n",
    "    # 计算两点之间的欧几里得距离\n",
    "    import math\n",
    "\n",
    "    return math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
    "\n",
    "\n",
    "def compute_angular_distance(seg1, seg2):\n",
    "    from math import atan2, sin\n",
    "\n",
    "    start1, end1 = seg1.points[0], seg1.points[-1]\n",
    "    start2, end2 = seg2.points[0], seg2.points[-1]\n",
    "    vector1 = (end1[0] - start1[0], end1[1] - start1[1])\n",
    "    vector2 = (end2[0] - start2[0], end2[1] - start2[1])\n",
    "    angle1 = atan2(vector1[1], vector1[0])\n",
    "    angle2 = atan2(vector2[1], vector2[0])\n",
    "    angle_diff = abs(angle1 - angle2)\n",
    "    if angle_diff > np.pi:\n",
    "        angle_diff = 2 * np.pi - angle_diff\n",
    "    len1 = compute_point_distance(start1, end1)\n",
    "    len2 = compute_point_distance(start2, end2)\n",
    "    return abs(sin(angle_diff)) * max(len1, len2)\n",
    "\n",
    "\n",
    "def compute_vector_distance(r1, r2):\n",
    "    sum_square = torch.sum((r1 - r2) ** 2)\n",
    "    return torch.sqrt(sum_square).item()\n",
    "\n",
    "\n",
    "def calculate_distance(seg1, seg2, alpha, beta, gamma):\n",
    "    d1 = compute_point_distance(\n",
    "        seg1.points[0], seg2.points[0]\n",
    "    ) + compute_point_distance(seg1.points[-1], seg2.points[-1])\n",
    "    d2 = compute_angular_distance(seg1, seg2)\n",
    "    d3 = compute_vector_distance(seg1.emb, seg2.emb)\n",
    "    return alpha * d1 + beta * d2 + gamma * d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, segments):\n",
    "        self.items = segments\n",
    "        self.size = len(segments)\n",
    "        self.centroid = self._calculate_centroid()\n",
    "        self.radius = self._calculate_radius()\n",
    "        self.merged = False\n",
    "\n",
    "    def _calculate_centroid(self):\n",
    "        total_x = 0\n",
    "        total_y = 0\n",
    "        for seg in self.items:\n",
    "            start, end = seg.points[0], seg.points[-1]\n",
    "            mid_x = (start[0] + end[0]) / 2\n",
    "            mid_y = (start[1] + end[1]) / 2\n",
    "            total_x += mid_x\n",
    "            total_y += mid_y\n",
    "        centroid_x = total_x / self.size\n",
    "        centroid_y = total_y / self.size\n",
    "        return (centroid_x, centroid_y)\n",
    "\n",
    "    def _calculate_radius(self):\n",
    "        max_distance = 0\n",
    "        for seg in self.items:\n",
    "            start, end = seg.points[0], seg.points[-1]\n",
    "            mid_x = (start[0] + end[0]) / 2\n",
    "            mid_y = (start[1] + end[1]) / 2\n",
    "            distance = compute_point_distance((mid_x, mid_y), self.centroid)\n",
    "            if distance > max_distance:\n",
    "                max_distance = distance\n",
    "        return max_distance\n",
    "\n",
    "\n",
    "def secure_set_union(local_cs: List[List[Cluster]]) -> List[Cluster]:\n",
    "    union_c = []\n",
    "    for local_c in local_cs:\n",
    "        union_c.extend(local_c)\n",
    "    return union_c\n",
    "\n",
    "\n",
    "def merge_clus(cluster1: Cluster, cluster2: Cluster) -> Cluster:\n",
    "    all_segments = cluster1.items + cluster2.items\n",
    "    return Cluster(all_segments)\n",
    "\n",
    "\n",
    "def local_clustering(trajectory_segments, eps, min_samples, alpha, beta, gamma):\n",
    "    from tqdm import tqdm\n",
    "    num_segments = len(trajectory_segments)\n",
    "    distance_matrix = np.zeros((num_segments, num_segments))\n",
    "    for i in tqdm(range(num_segments)):\n",
    "        for j in range(i + 1, num_segments):\n",
    "            dist = calculate_distance(\n",
    "                trajectory_segments[i], trajectory_segments[j], alpha, beta, gamma\n",
    "            )\n",
    "            distance_matrix[i, j] = dist\n",
    "            distance_matrix[j, i] = dist\n",
    "\n",
    "    print(\"distance matrix calculated\")\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=\"precomputed\")\n",
    "    labels = db.fit_predict(distance_matrix)\n",
    "\n",
    "    local_clusters = []\n",
    "    unique_labels = set(labels)\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            continue\n",
    "        cluster_indices = np.where(labels == label)[0]\n",
    "        cluster_segments = [trajectory_segments[i] for i in cluster_indices]\n",
    "        cluster = Cluster(cluster_segments)\n",
    "        local_clusters.append(cluster)\n",
    "\n",
    "    return local_clusters\n",
    "\n",
    "\n",
    "def federated_cluster_aggregation(\n",
    "    local_cs: List[List[Cluster]], eps\n",
    ") -> List[Cluster]:\n",
    "    global_c = secure_set_union(local_cs)\n",
    "    dset = []\n",
    "    for i in range(len(global_c)):\n",
    "        global_c[i].merged = False\n",
    "        for j in range(i + 1, len(global_c)):\n",
    "            d_ij = compute_point_distance(global_c[i].centroid, global_c[j].centroid)\n",
    "            dset.append((global_c[i], global_c[j], d_ij))\n",
    "    dset.sort(key=lambda x: x[2])\n",
    "    for c_i, c_j, d_ij in dset:\n",
    "        if not c_i.merged and not c_j.merged and d_ij < eps:\n",
    "            c_merged = merge_clus(c_i, c_j)\n",
    "            c_i.merged = True\n",
    "            c_j.merged = True\n",
    "            global_c.append(c_merged)\n",
    "            global_c.remove(c_i)\n",
    "            global_c.remove(c_j)\n",
    "    return global_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2086/2086 [05:39<00:00,  6.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance matrix calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2005/2005 [05:04<00:00,  6.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance matrix calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2204/2204 [05:46<00:00,  6.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance matrix calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2385/2385 [07:22<00:00,  5.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance matrix calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2696/2696 [09:50<00:00,  4.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance matrix calculated\n",
      "Silhouette Coefficient: -0.40888230456470276\n",
      "Cluster 0:\n",
      "  Centroid: (-959767.1643883036, 5035739.946823297)\n",
      "  Radius: 6626.255833811956\n",
      "  Size: 2387\n",
      "Cluster 1:\n",
      "  Centroid: (-962532.270446104, 5037505.370819499)\n",
      "  Radius: 101.51792743408294\n",
      "  Size: 2\n",
      "Cluster 2:\n",
      "  Centroid: (-965959.0517270409, 5037627.078805006)\n",
      "  Radius: 941.30169072967\n",
      "  Size: 15\n",
      "Cluster 3:\n",
      "  Centroid: (-966294.4796166988, 5037871.489096412)\n",
      "  Radius: 197.32909623178105\n",
      "  Size: 3\n",
      "Cluster 4:\n",
      "  Centroid: (-967114.5298255578, 5037941.165863165)\n",
      "  Radius: 331.19733776888734\n",
      "  Size: 11\n",
      "Cluster 5:\n",
      "  Centroid: (-959244.6162647608, 5038731.238111893)\n",
      "  Radius: 57.12107175323838\n",
      "  Size: 2\n",
      "Cluster 6:\n",
      "  Centroid: (-966909.5893780134, 5037123.94481836)\n",
      "  Radius: 953.2884516357527\n",
      "  Size: 12\n",
      "Cluster 7:\n",
      "  Centroid: (-966609.5565908328, 5036043.820925992)\n",
      "  Radius: 833.2392279325032\n",
      "  Size: 13\n",
      "Cluster 8:\n",
      "  Centroid: (-956180.8812391484, 5037484.401423391)\n",
      "  Radius: 13.144847550152443\n",
      "  Size: 2\n",
      "Cluster 9:\n",
      "  Centroid: (-961915.6161268547, 5033831.47048375)\n",
      "  Radius: 270.9185236270771\n",
      "  Size: 7\n",
      "Cluster 10:\n",
      "  Centroid: (-961593.5847427741, 5033348.812318342)\n",
      "  Radius: 384.58303970126434\n",
      "  Size: 14\n",
      "Cluster 11:\n",
      "  Centroid: (-964036.586384939, 5039447.076009141)\n",
      "  Radius: 59.865900199704924\n",
      "  Size: 2\n",
      "Cluster 12:\n",
      "  Centroid: (-964115.033230101, 5040024.436730576)\n",
      "  Radius: 349.3003947399362\n",
      "  Size: 5\n",
      "Cluster 13:\n",
      "  Centroid: (-962204.7824191267, 5037402.463998771)\n",
      "  Radius: 379.2764060014245\n",
      "  Size: 8\n",
      "Cluster 14:\n",
      "  Centroid: (-961254.1278826883, 5037474.089594755)\n",
      "  Radius: 147.53995409300887\n",
      "  Size: 2\n",
      "Cluster 15:\n",
      "  Centroid: (-953597.2950072, 5035002.954451581)\n",
      "  Radius: 158.27933794363378\n",
      "  Size: 6\n",
      "Cluster 16:\n",
      "  Centroid: (-954533.7980533711, 5038986.482614261)\n",
      "  Radius: 26.759757465727358\n",
      "  Size: 2\n",
      "Cluster 17:\n",
      "  Centroid: (-956953.0767169087, 5033165.791503584)\n",
      "  Radius: 23.303098343804336\n",
      "  Size: 2\n",
      "Cluster 18:\n",
      "  Centroid: (-956161.3446685141, 5033262.909370631)\n",
      "  Radius: 58.23172840312147\n",
      "  Size: 2\n",
      "Cluster 19:\n",
      "  Centroid: (-954554.0860305682, 5035008.514764697)\n",
      "  Radius: 113.41859131948223\n",
      "  Size: 2\n",
      "Cluster 20:\n",
      "  Centroid: (-953484.5840227718, 5035539.755931854)\n",
      "  Radius: 97.94934876540367\n",
      "  Size: 2\n",
      "Cluster 21:\n",
      "  Centroid: (-953704.579166452, 5039691.005636901)\n",
      "  Radius: 730.6894850048404\n",
      "  Size: 15\n",
      "Cluster 22:\n",
      "  Centroid: (-955626.8441334702, 5035354.128250007)\n",
      "  Radius: 76.20137109871183\n",
      "  Size: 2\n",
      "Cluster 23:\n",
      "  Centroid: (-957987.0121473967, 5029559.233610488)\n",
      "  Radius: 779.9861208783005\n",
      "  Size: 6\n",
      "Cluster 24:\n",
      "  Centroid: (-956446.7539279718, 5028000.095297949)\n",
      "  Radius: 387.99831935208806\n",
      "  Size: 4\n",
      "Cluster 25:\n",
      "  Centroid: (-959575.7360901255, 5031068.245371473)\n",
      "  Radius: 434.674940064683\n",
      "  Size: 11\n",
      "Cluster 26:\n",
      "  Centroid: (-960245.364572056, 5031588.725308564)\n",
      "  Radius: 836.3858260682322\n",
      "  Size: 8\n",
      "Cluster 27:\n",
      "  Centroid: (-966012.6604108193, 5039727.12849281)\n",
      "  Radius: 172.42295750610276\n",
      "  Size: 4\n",
      "Cluster 28:\n",
      "  Centroid: (-963237.465505343, 5039656.741306245)\n",
      "  Radius: 299.5407667608491\n",
      "  Size: 4\n",
      "Cluster 29:\n",
      "  Centroid: (-954818.3306718387, 5038969.510298604)\n",
      "  Radius: 119.22735459399568\n",
      "  Size: 2\n",
      "Cluster 30:\n",
      "  Centroid: (-953798.5328166815, 5033029.505773428)\n",
      "  Radius: 395.92989799047143\n",
      "  Size: 9\n",
      "Cluster 31:\n",
      "  Centroid: (-960952.3129132751, 5033004.8277598405)\n",
      "  Radius: 135.0330608157814\n",
      "  Size: 2\n",
      "Cluster 32:\n",
      "  Centroid: (-961817.9332736837, 5038628.411148453)\n",
      "  Radius: 302.6417087145386\n",
      "  Size: 3\n",
      "Cluster 33:\n",
      "  Centroid: (-950035.3774304151, 5032468.713320102)\n",
      "  Radius: 135.76121762253675\n",
      "  Size: 3\n",
      "Cluster 34:\n",
      "  Centroid: (-954551.0804043169, 5041022.022358814)\n",
      "  Radius: 183.764818151118\n",
      "  Size: 2\n",
      "Cluster 35:\n",
      "  Centroid: (-954611.8987961162, 5041807.112721928)\n",
      "  Radius: 485.6667231317814\n",
      "  Size: 11\n",
      "Cluster 36:\n",
      "  Centroid: (-967891.8029900917, 5039902.855758609)\n",
      "  Radius: 176.2701718940149\n",
      "  Size: 4\n",
      "Cluster 37:\n",
      "  Centroid: (-953085.7541171322, 5033169.559850686)\n",
      "  Radius: 271.47458581228267\n",
      "  Size: 3\n",
      "Cluster 38:\n",
      "  Centroid: (-952450.6485922838, 5033452.4858985)\n",
      "  Radius: 88.9361443523848\n",
      "  Size: 2\n"
     ]
    }
   ],
   "source": [
    "# 本地聚类参数\n",
    "local_eps = 1000.0\n",
    "local_min_samples = 2\n",
    "alpha = 1\n",
    "beta = 1\n",
    "gamma = 1\n",
    "\n",
    "# 联邦聚类聚合参数\n",
    "federated_eps = 3.0\n",
    "\n",
    "# 模拟多个参与方的本地聚类\n",
    "for segments in fed_segments:\n",
    "    local_clustering_results = []\n",
    "    local_clusters = local_clustering(\n",
    "        segments, local_eps, local_min_samples, alpha, beta, gamma\n",
    "    )\n",
    "    local_clustering_results.append(local_clusters)\n",
    "\n",
    "# 进行联邦聚类聚合\n",
    "global_clusters = federated_cluster_aggregation(local_clustering_results, federated_eps)\n",
    "\n",
    "# 计算轮廓系数\n",
    "from sklearn.metrics import silhouette_score\n",
    "all_segments = []\n",
    "labels = []\n",
    "for i, cluster in enumerate(global_clusters):\n",
    "    for segment in cluster.items:\n",
    "        all_segments.append(segment)\n",
    "        labels.append(i)\n",
    "\n",
    "num_segments = len(all_segments)\n",
    "distance_matrix = np.zeros((num_segments, num_segments))\n",
    "for i in range(num_segments):\n",
    "    for j in range(i + 1, num_segments):\n",
    "        dist = calculate_distance(\n",
    "            all_segments[i], all_segments[j], alpha, beta, gamma\n",
    "        )\n",
    "        distance_matrix[i, j] = dist\n",
    "        distance_matrix[j, i] = dist\n",
    "\n",
    "silhouette_avg = silhouette_score(distance_matrix, labels, metric='precomputed')\n",
    "print(f\"Silhouette Coefficient: {silhouette_avg}\")\n",
    "\n",
    "# 输出结果\n",
    "for i, cluster in enumerate(global_clusters):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(f\"  Centroid: {cluster.centroid}\")\n",
    "    print(f\"  Radius: {cluster.radius}\")\n",
    "    print(f\"  Size: {cluster.size}\")\n",
    "    # print(f\"  Items: {cluster.items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
