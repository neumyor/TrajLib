{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import traj_dist.distance as tdist\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "from fedtraj.config import Config\n",
    "from fedtraj.utils import tool_funcs\n",
    "from fedtraj.utils.cellspace import CellSpace\n",
    "from fedtraj.utils.tool_funcs import lonlat2meters\n",
    "from fedtraj.model.layers.node2vec_ import train_node2vec\n",
    "from fedtraj.utils.edwp import edwp\n",
    "from fedtraj.utils.data_loader import read_trajsimi_traj_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format=\"[%(filename)s:%(lineno)s %(funcName)s()] -> %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(\n",
    "                Config.root_dir + \"/exp/log/\" + tool_funcs.log_file_name(), mode=\"w\"\n",
    "            ),\n",
    "            logging.StreamHandler(),\n",
    "        ],\n",
    "    )\n",
    "Config.dataset = \"porto\"\n",
    "Config.post_value_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inrange(lon, lat):\n",
    "    if (\n",
    "        lon <= Config.min_lon\n",
    "        or lon >= Config.max_lon\n",
    "        or lat <= Config.min_lat\n",
    "        or lat >= Config.max_lat\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean_and_output_data():\n",
    "    _time = time.time()\n",
    "    # https://archive.ics.uci.edu/ml/machine-learning-databases/00339/\n",
    "    # download train.csv.zip and unzip it. rename train.csv to porto.csv\n",
    "    dfraw = pd.read_csv(r\"D:\\codes\\TrajMM\\resource\\dataset\\Porto\\porto_sample.csv\")\n",
    "    dfraw = dfraw.rename(columns={\"POLYLINE\": \"wgs_seq\"})\n",
    "\n",
    "    dfraw = dfraw[dfraw.MISSING_DATA == False]\n",
    "\n",
    "    # length requirement\n",
    "    dfraw.wgs_seq = dfraw.wgs_seq.apply(literal_eval)\n",
    "    dfraw[\"trajlen\"] = dfraw.wgs_seq.apply(lambda traj: len(traj))\n",
    "    dfraw = dfraw[\n",
    "        (dfraw.trajlen >= Config.min_traj_len) & (dfraw.trajlen <= Config.max_traj_len)\n",
    "    ]\n",
    "    logging.info(\"Preprocessed-rm length. #traj={}\".format(dfraw.shape[0]))\n",
    "\n",
    "    # range requirement\n",
    "    dfraw[\"inrange\"] = dfraw.wgs_seq.map(\n",
    "        lambda traj: sum([inrange(p[0], p[1]) for p in traj]) == len(traj)\n",
    "    )  # True: valid\n",
    "    dfraw = dfraw[dfraw.inrange == True]\n",
    "    logging.info(\"Preprocessed-rm range. #traj={}\".format(dfraw.shape[0]))\n",
    "\n",
    "    # convert to Mercator\n",
    "    dfraw[\"merc_seq\"] = dfraw.wgs_seq.apply(\n",
    "        lambda traj: [list(lonlat2meters(p[0], p[1])) for p in traj]\n",
    "    )\n",
    "\n",
    "    logging.info(\"Preprocessed-output. #traj={}\".format(dfraw.shape[0]))\n",
    "    dfraw = dfraw[[\"trajlen\", \"wgs_seq\", \"merc_seq\"]].reset_index(drop=True)\n",
    "\n",
    "    dfraw.to_pickle(Config.dataset_file)\n",
    "    logging.info(\"Preprocess end. @={:.0f}\".format(time.time() - _time))\n",
    "    return\n",
    "\n",
    "\n",
    "def init_cellspace():\n",
    "    # 1. create cellspase\n",
    "    # 2. initialize cell embeddings (create graph, train, and dump to file)\n",
    "\n",
    "    x_min, y_min = lonlat2meters(Config.min_lon, Config.min_lat)\n",
    "    x_max, y_max = lonlat2meters(Config.max_lon, Config.max_lat)\n",
    "    x_min -= Config.cellspace_buffer\n",
    "    y_min -= Config.cellspace_buffer\n",
    "    x_max += Config.cellspace_buffer\n",
    "    y_max += Config.cellspace_buffer\n",
    "\n",
    "    cell_size = int(Config.cell_size)\n",
    "    cs = CellSpace(cell_size, cell_size, x_min, y_min, x_max, y_max)\n",
    "    with open(Config.dataset_cell_file, \"wb\") as fh:\n",
    "        pickle.dump(cs, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    _, edge_index = cs.all_neighbour_cell_pairs_permutated_optmized()\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long, device=Config.device).T\n",
    "    train_node2vec(edge_index)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1446887690.py:27 clean_and_output_data()] -> Preprocessed-rm length. #traj=979\n",
      "[1446887690.py:34 clean_and_output_data()] -> Preprocessed-rm range. #traj=819\n",
      "[1446887690.py:41 clean_and_output_data()] -> Preprocessed-output. #traj=819\n",
      "[1446887690.py:45 clean_and_output_data()] -> Preprocess end. @=0\n",
      "[node2vec_.py:17 train_node2vec()] -> [node2vec] start.\n",
      "100%|██████████| 2056/2056 [00:33<00:00, 60.54it/s]\n",
      "[node2vec_.py:83 train_node2vec()] -> [node2vec] i_ep=0, loss=2.8100 @=33.96432614326477\n",
      "100%|██████████| 2056/2056 [00:29<00:00, 70.54it/s]\n",
      "[node2vec_.py:83 train_node2vec()] -> [node2vec] i_ep=1, loss=1.2097 @=29.150488138198853\n",
      "100%|██████████| 2056/2056 [00:28<00:00, 73.05it/s]\n",
      "[node2vec_.py:83 train_node2vec()] -> [node2vec] i_ep=2, loss=0.8822 @=28.149784564971924\n",
      "100%|██████████| 2056/2056 [00:28<00:00, 71.90it/s]\n",
      "[node2vec_.py:83 train_node2vec()] -> [node2vec] i_ep=3, loss=0.8013 @=28.59784746170044\n",
      "100%|██████████| 2056/2056 [00:29<00:00, 70.64it/s]\n",
      "[node2vec_.py:83 train_node2vec()] -> [node2vec] i_ep=4, loss=0.7765 @=29.107580184936523\n",
      "100%|██████████| 2056/2056 [00:29<00:00, 70.23it/s]\n",
      "[node2vec_.py:83 train_node2vec()] -> [node2vec] i_ep=5, loss=0.7692 @=29.278178930282593\n",
      "100%|██████████| 2056/2056 [00:28<00:00, 72.08it/s]\n",
      "[node2vec_.py:83 train_node2vec()] -> [node2vec] i_ep=6, loss=0.7666 @=28.527326107025146\n",
      "100%|██████████| 2056/2056 [00:28<00:00, 71.52it/s]\n",
      "[node2vec_.py:83 train_node2vec()] -> [node2vec] i_ep=7, loss=0.7651 @=28.750574111938477\n",
      "d:\\codes\\TrajMM\\fedtraj\\model\\layers\\node2vec_.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file)\n",
      "[node2vec_.py:70 save_embeddings()] -> [save embedding] done.\n",
      "[node2vec_.py:101 train_node2vec()] -> [node2vec] @=236, best_ep=7\n"
     ]
    }
   ],
   "source": [
    "clean_and_output_data()\n",
    "init_cellspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_newsimi_test_dataset():\n",
    "    trajs = pd.read_pickle(Config.dataset_file)  # using test part only\n",
    "    l = trajs.shape[0]\n",
    "    n_query = 1000\n",
    "    n_db = 100000\n",
    "    test_idx = (int(l * 0.8), int(l * 0.8) + n_db)\n",
    "    test_trajs = trajs[test_idx[0] : test_idx[1]]\n",
    "    logging.info(\"Test trajs loaded.\")\n",
    "\n",
    "    # for varying db size\n",
    "    def _raw_dataset():\n",
    "        query_lst = []  # [N, len, 2]\n",
    "        db_lst = []\n",
    "        i = 0\n",
    "        for _, v in test_trajs.merc_seq.items():\n",
    "            if i < n_query:\n",
    "                query_lst.append(np.array(v)[::2].tolist())\n",
    "            db_lst.append(np.array(v)[1::2].tolist())\n",
    "            i += 1\n",
    "\n",
    "        output_file_name = Config.dataset_file + \"_newsimi_raw.pkl\"\n",
    "        with open(output_file_name, \"wb\") as fh:\n",
    "            pickle.dump((query_lst, db_lst), fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            logging.info(\"_raw_dataset done.\")\n",
    "        return\n",
    "\n",
    "    # for varying downsampling rate\n",
    "    def _downsample_dataset(rate):\n",
    "        unrate = 1 - rate  # preserved rate\n",
    "        query_lst = []  # [N, len, 2]\n",
    "        db_lst = []\n",
    "        i = 0\n",
    "        for _, v in test_trajs.merc_seq.items():\n",
    "            if i < n_query:\n",
    "                _q = np.array(v)[::2]\n",
    "                _q_len = _q.shape[0]\n",
    "                _idx = np.sort(\n",
    "                    np.random.choice(_q_len, math.ceil(_q_len * unrate), replace=False)\n",
    "                )\n",
    "                query_lst.append(_q[_idx].tolist())\n",
    "            _db = np.array(v)[1::2]\n",
    "            _db_len = _db.shape[0]\n",
    "            _idx = np.sort(\n",
    "                np.random.choice(_db_len, math.ceil(_db_len * unrate), replace=False)\n",
    "            )\n",
    "            db_lst.append(_db[_idx].tolist())\n",
    "            i += 1\n",
    "\n",
    "        output_file_name = (\n",
    "            Config.dataset_file + \"_newsimi_downsampling_\" + str(rate) + \".pkl\"\n",
    "        )\n",
    "        with open(output_file_name, \"wb\") as fh:\n",
    "            pickle.dump((query_lst, db_lst), fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            logging.info(\"_downsample_dataset done. rate={}\".format(rate))\n",
    "        return\n",
    "\n",
    "    # for varying distort rate\n",
    "    def _distort_dataset(rate):\n",
    "        query_lst = []  # [N, len, 2]\n",
    "        db_lst = []\n",
    "        i = 0\n",
    "        for _, v in test_trajs.merc_seq.items():\n",
    "            if i < n_query:\n",
    "                _q = np.array(v)[::2]\n",
    "                for _row in range(_q.shape[0]):\n",
    "                    if random.random() < rate:\n",
    "                        _q[_row] = _q[_row] + [\n",
    "                            tool_funcs.truncated_rand(),\n",
    "                            tool_funcs.truncated_rand(),\n",
    "                        ]\n",
    "                query_lst.append(_q.tolist())\n",
    "\n",
    "            _db = np.array(v)[1::2]\n",
    "            for _row in range(_db.shape[0]):\n",
    "                if random.random() < rate:\n",
    "                    _db[_row] = _db[_row] + [\n",
    "                        tool_funcs.truncated_rand(),\n",
    "                        tool_funcs.truncated_rand(),\n",
    "                    ]\n",
    "            db_lst.append(_db.tolist())\n",
    "            i += 1\n",
    "\n",
    "        output_file_name = (\n",
    "            Config.dataset_file + \"_newsimi_distort_\" + str(rate) + \".pkl\"\n",
    "        )\n",
    "        with open(output_file_name, \"wb\") as fh:\n",
    "            pickle.dump((query_lst, db_lst), fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            logging.info(\"_distort_dataset done. rate={}\".format(rate))\n",
    "        return\n",
    "\n",
    "    _raw_dataset()\n",
    "\n",
    "    for rate in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "        _downsample_dataset(rate)\n",
    "\n",
    "    for rate in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "        _distort_dataset(rate)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# ===calculate trajsimi distance matrix for trajsimi learning===\n",
    "def traj_simi_computation(fn_name=\"hausdorff\"):\n",
    "    # 1. read trajs from file, and split to 3 datasets, and data normalization\n",
    "    # 2. calculate simi in 3 datasets separately.\n",
    "    # 3. dump 3 datasets into files\n",
    "    logging.info(\"traj_simi_computation starts. fn={}\".format(fn_name))\n",
    "    _time = time.time()\n",
    "\n",
    "    # 1.\n",
    "    trains, evals, tests = read_trajsimi_traj_dataset(Config.dataset_file)\n",
    "    trains, evals, tests = _normalization([trains, evals, tests])\n",
    "\n",
    "    logging.info(\n",
    "        \"traj dataset sizes. traj: trains/evals/tests={}/{}/{}\".format(\n",
    "            trains.shape[0], evals.shape[0], tests.shape[0]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2.\n",
    "    fn = _get_simi_fn(fn_name)\n",
    "    tests_simi = _simi_matrix(fn, tests)\n",
    "    evals_simi = _simi_matrix(fn, evals)\n",
    "    trains_simi = _simi_matrix(fn, trains)  # [ [simi, simi, ... ], ... ]\n",
    "\n",
    "    max_distance = max(\n",
    "        max(map(max, trains_simi)), max(map(max, evals_simi)), max(map(max, tests_simi))\n",
    "    )\n",
    "\n",
    "    _output_file = \"{}_traj_simi_dict_{}.pkl\".format(Config.dataset_file, fn_name)\n",
    "    with open(_output_file, \"wb\") as fh:\n",
    "        tup = trains_simi, evals_simi, tests_simi, max_distance\n",
    "        pickle.dump(tup, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    logging.info(\"traj_simi_computation ends. @={:.3f}\".format(time.time() - _time))\n",
    "    return tup\n",
    "\n",
    "\n",
    "def _normalization(lst_df):\n",
    "    # lst_df: [df, df, df]\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for df in lst_df:\n",
    "        for _, v in df.merc_seq.items():\n",
    "            arr = np.array(v)\n",
    "            xs.append(arr[:, 0])\n",
    "            ys.append(arr[:, 1])\n",
    "\n",
    "    xs = np.concatenate(xs)\n",
    "    ys = np.concatenate(ys)\n",
    "    mean = np.array([xs.mean(), ys.mean()])\n",
    "    std = np.array([xs.std(), ys.std()])\n",
    "\n",
    "    for i in range(len(lst_df)):\n",
    "        lst_df[i].merc_seq = lst_df[i].merc_seq.apply(\n",
    "            lambda lst: ((np.array(lst) - mean) / std).tolist()\n",
    "        )\n",
    "\n",
    "    return lst_df\n",
    "\n",
    "\n",
    "def _get_simi_fn(fn_name):\n",
    "    fn = {\n",
    "        \"lcss\": tdist.lcss,\n",
    "        \"edr\": tdist.edr,\n",
    "        \"frechet\": tdist.frechet,\n",
    "        \"discret_frechet\": tdist.discret_frechet,\n",
    "        \"hausdorff\": tdist.hausdorff,\n",
    "        \"edwp\": edwp,\n",
    "    }.get(fn_name, None)\n",
    "    if fn_name == \"lcss\" or fn_name == \"edr\":\n",
    "        fn = partial(fn, eps=Config.test_exp1_lcss_edr_epsilon)\n",
    "    return fn\n",
    "\n",
    "\n",
    "def _simi_matrix(fn, df):\n",
    "    _time = time.time()\n",
    "\n",
    "    l = df.shape[0]\n",
    "    batch_size = 50\n",
    "    assert l % batch_size == 0\n",
    "\n",
    "    # parallel init\n",
    "    tasks = []\n",
    "    for i in range(math.ceil(l / batch_size)):\n",
    "        if i < math.ceil(l / batch_size) - 1:\n",
    "            tasks.append((fn, df, list(range(batch_size * i, batch_size * (i + 1)))))\n",
    "        else:\n",
    "            tasks.append((fn, df, list(range(batch_size * i, l))))\n",
    "\n",
    "    num_cores = int(mp.cpu_count()) - 4\n",
    "    # num_cores = 10\n",
    "    logging.info(\"pool.size={}\".format(num_cores))\n",
    "    assert num_cores > 0\n",
    "    pool = mp.Pool(num_cores)\n",
    "    lst_simi = pool.starmap(_simi_comp_operator, tasks)\n",
    "    pool.close()\n",
    "\n",
    "    # extend lst_simi to matrix simi and pad 0s\n",
    "    lst_simi = sum(lst_simi, [])\n",
    "    for i, row_simi in enumerate(lst_simi):\n",
    "        lst_simi[i] = [0] * (i + 1) + row_simi\n",
    "    assert sum(map(len, lst_simi)) == l**2\n",
    "    logging.info(\n",
    "        \"simi_matrix computation done., @={}, #={}\".format(\n",
    "            time.time() - _time, len(lst_simi)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return lst_simi\n",
    "\n",
    "\n",
    "# async operator\n",
    "def _simi_comp_operator(fn, df_trajs, sub_idx):\n",
    "    simi = []\n",
    "    l = df_trajs.shape[0]\n",
    "    for _i in sub_idx:\n",
    "        t_i = np.array(df_trajs.iloc[_i].merc_seq)\n",
    "        simi_row = []\n",
    "        for _j in range(_i + 1, l):\n",
    "            t_j = np.array(df_trajs.iloc[_j].merc_seq)\n",
    "            simi_row.append(float(fn(t_i, t_j)))\n",
    "        simi.append(simi_row)\n",
    "    logging.debug(\n",
    "        \"simi_comp_operator ends. sub_idx=[{}:{}], pid={}\".format(\n",
    "            sub_idx[0], sub_idx[-1], os.getpid()\n",
    "        )\n",
    "    )\n",
    "    return simi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[840959107.py:8 generate_newsimi_test_dataset()] -> Test trajs loaded.\n",
      "[840959107.py:24 _raw_dataset()] -> _raw_dataset done.\n",
      "[840959107.py:54 _downsample_dataset()] -> _downsample_dataset done. rate=0.1\n",
      "[840959107.py:54 _downsample_dataset()] -> _downsample_dataset done. rate=0.2\n",
      "[840959107.py:54 _downsample_dataset()] -> _downsample_dataset done. rate=0.3\n",
      "[840959107.py:54 _downsample_dataset()] -> _downsample_dataset done. rate=0.4\n",
      "[840959107.py:54 _downsample_dataset()] -> _downsample_dataset done. rate=0.5\n",
      "[840959107.py:88 _distort_dataset()] -> _distort_dataset done. rate=0.1\n",
      "[840959107.py:88 _distort_dataset()] -> _distort_dataset done. rate=0.2\n",
      "[840959107.py:88 _distort_dataset()] -> _distort_dataset done. rate=0.3\n",
      "[840959107.py:88 _distort_dataset()] -> _distort_dataset done. rate=0.4\n",
      "[840959107.py:88 _distort_dataset()] -> _distort_dataset done. rate=0.5\n",
      "[840959107.py:107 traj_simi_computation()] -> traj_simi_computation starts. fn=lcss\n",
      "[data_loader.py:52 read_trajsimi_traj_dataset()] -> [Load trajsimi traj dataset] START.\n",
      "[data_loader.py:70 read_trajsimi_traj_dataset()] -> trajsimi traj dataset sizes. traj: #total=246 (trains/evals/tests=172/24/50)\n",
      "[840959107.py:114 traj_simi_computation()] -> traj dataset sizes. traj: trains/evals/tests=172/24/50\n",
      "[840959107.py:193 _simi_matrix()] -> pool.size=12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819\n"
     ]
    }
   ],
   "source": [
    "generate_newsimi_test_dataset()\n",
    "traj_simi_computation(\"lcss\")  # edr edwp discret_frechet hausdorff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
